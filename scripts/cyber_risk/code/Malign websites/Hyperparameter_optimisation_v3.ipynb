{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d99e75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from hyperopt import space_eval\n",
    "\n",
    "#algorithms\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "df=pd.read_csv(r'C:\\Users\\isarachchand\\\\Documents\\git\\apf\\datasets\\cyber_risk\\data\\malign_websites_dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b293d465",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prep the data\n",
    "\n",
    "100 * df['Type'].value_counts()/len(df)   #variable imbalance\n",
    "\n",
    "#unique categories for each categorical column\n",
    "\n",
    "for i in df.select_dtypes(include='object').columns:\n",
    "    print(f\"{i} -> {df[i].nunique()}\")\n",
    "    \n",
    "df['CHARSET'].value_counts()\n",
    "\n",
    "# Top 5 categories kept\n",
    "\n",
    "def CHARSET_CLEANER(x):\n",
    "    if x not in ['UTF-8','ISO-8859-1','utf-8','us-ascii','iso-8859-1']:\n",
    "        return \"OTHERS\"\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "df['CHARSET'] = df['CHARSET'].apply(CHARSET_CLEANER)\n",
    "df['CHARSET'].value_counts()\n",
    "df['SERVER'].value_counts()\n",
    "\n",
    "# Top 5 categories kept\n",
    "\n",
    "def SERVER_CLEANER(x):\n",
    "    if x not in ['Apache','nginx','None','Microsoft-HTTPAPI/2.0','cloudflare-nginx']:\n",
    "        return \"OTHERS\"\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "df['SERVER'] = df['SERVER'].apply(SERVER_CLEANER)\n",
    "df['SERVER'].value_counts()\n",
    "df['WHOIS_STATEPRO'].value_counts()[:11]\n",
    "\n",
    "def STATE_CLEANER(x):\n",
    "    if x not in ['CA','None','NY','WA','Barcelona','FL']:\n",
    "        return \"OTHERS\"\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "df['WHOIS_STATEPRO'] = df['WHOIS_STATEPRO'].apply(STATE_CLEANER)\n",
    "df['WHOIS_STATEPRO'].value_counts()\n",
    "\n",
    "def DATE_CLEANER(x):\n",
    "    if x == 'None':\n",
    "        return \"Absent\"\n",
    "    else:\n",
    "        return \"Present\"\n",
    "df['WHOIS_REGDATE'] = df['WHOIS_REGDATE'].apply(DATE_CLEANER)\n",
    "df['WHOIS_UPDATED_DATE'] = df['WHOIS_UPDATED_DATE'].apply(DATE_CLEANER)\n",
    "\n",
    "df.drop(['URL','WHOIS_COUNTRY','CONTENT_LENGTH'],axis=1,inplace=True)\n",
    "# change null values to 0\n",
    "\n",
    "df = df.fillna(0)\n",
    "\n",
    "le = LabelEncoder()\n",
    "for column in ['CHARSET','SERVER', 'WHOIS_STATEPRO', 'WHOIS_REGDATE', 'WHOIS_UPDATED_DATE']:\n",
    "    df[column] = le.fit_transform(df[column].astype(str))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536241f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = []\n",
    "accuracy_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a172c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "y = df['Type']\n",
    "X = df.drop('Type', axis=1)\n",
    "\n",
    "best_index = 1\n",
    "partition_count = 5\n",
    "\n",
    "# get train test split\n",
    "partition_size = math.ceil(len(X) / partition_count)\n",
    "test_start = best_index * partition_size\n",
    "test_end = test_start + partition_size\n",
    "test_x = X[test_start:test_end]\n",
    "test_y = y[test_start:test_end]\n",
    "train_x = pd.concat([X[:test_start],X[test_end:]])\n",
    "train_y =  pd.concat([y[:test_start], y[test_end:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b29434d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Search Space\n",
    "space = hp.choice('classifiers', [\n",
    "    {\n",
    "    'model':KNeighborsClassifier(),\n",
    "    'params':{\n",
    "        'model__n_neighbors': hp.choice('knc.n_neighbors', range(2,10)),\n",
    "        'model__algorithm': hp.choice('knc.algorithm',\n",
    "                                      ['auto', 'ball_tree', 'kd_tree']),\n",
    "        'model__metric': hp.choice('knc.metric', ['chebyshev', 'minkowski'])\n",
    "    }\n",
    "    },\n",
    "    {\n",
    "    'model':SVC(),\n",
    "    'params':{\n",
    "        'model__C': hp.choice('C', np.arange(0.005,1.0,0.01)),\n",
    "        'model__kernel': hp.choice('kernel',['linear', 'rbf', 'sigmoid']),\n",
    "        'model__degree':hp.choice('degree',[2,3,4]),\n",
    "        'model__gamma': hp.uniform('gamma',0.001,1000)\n",
    "    }\n",
    "    },\n",
    "\n",
    "    {\n",
    "    'model': LogisticRegression(verbose=0),\n",
    "    'params': {\n",
    "        'model__penalty': hp.choice('lr.penalty', ['none', 'l2']),\n",
    "        'model__C': hp.choice('lr.C', np.arange(0.005,1.0,0.01))\n",
    "\n",
    "    }\n",
    "    },\n",
    "        {\n",
    "    'model': xgb.XGBClassifier(eval_metric='logloss', verbosity=0),\n",
    "    'params': {\n",
    "        'model__max_depth' : hp.choice('xgb.max_depth',\n",
    "                                       range(5, 30, 1)),\n",
    "        'model__learning_rate' : hp.quniform('xgb.learning_rate',\n",
    "                                             0.01, 0.5, 0.01),\n",
    "        'model__n_estimators' : hp.choice('xgb.n_estimators',\n",
    "                                          range(5, 50, 1)),\n",
    "        'model__reg_lambda' : hp.uniform ('xgb.reg_lambda', 0,1),\n",
    "        'model__reg_alpha' : hp.uniform ('xgb.reg_alpha', 0,1)\n",
    "    }\n",
    "    },\n",
    "#     {\n",
    "#     'model': RandomForestClassifier(), # Default params\n",
    "#     'params': {\n",
    "#     'max_depth': hp.quniform(\"max_depth\", 10, 180, 1),\n",
    "#     'min_sample_leaf' : hp.uniform('min_samples_leaf',1,5),\n",
    "#     'min_samples_split':hp.uniform('min_samples_split',2,6),\n",
    "#     'n_estimators': hp.uniform('n_estimators', 200, 900),\n",
    "#     'max_features':hp.choice('max_features',['sqrt', 'log2'])\n",
    "#     }\n",
    "#     },\n",
    "#     {\n",
    "#     'model': DecisionTreeClassifier(),\n",
    "#     'params': {\n",
    "#         'max_depth': hp.choice('max_depth', range(1,20)),\n",
    "#         'max_features': hp.choice('max_features', range(1,5)),\n",
    "#         'criterion': hp.choice('criterion', [\"gini\", \"entropy\"]),\n",
    "#         'min_sample_leaf' : hp.uniform('min_samples_leaf',1,5),\n",
    "#         'min_samples_split':hp.uniform('min_samples_split',2,6),\n",
    "    \n",
    "#         }\n",
    "        \n",
    "#     }\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6288b222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Objective function whose loss we have to minimize\n",
    "def objective(args):\n",
    "    \n",
    "    # Initialize model pipeline\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('model', args[\"model\"]) # args[model] will be sent by fmin from search space\n",
    "    ])\n",
    "    \n",
    "    pipe.set_params(**args['params']) # Model parameters will be set here\n",
    "    \n",
    "    # Cross Validation Score. Note the transformer.fit_transform for X_train. \n",
    "    \n",
    "    score = cross_val_score(pipe, train_x, train_y, cv=5, n_jobs=-1, error_score=0.99)\n",
    "    #accuracy = accuracy_score(pred, test_y)\n",
    "    print(f\"Model Name: {args['model']}: \", score)\n",
    "          \n",
    "    # Since we have to minimize the score, we return 1- score.\n",
    "    return {'loss': 1 - np.median(score), 'status': STATUS_OK}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d189355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperopts Trials() records all the model and run artifacts.\n",
    "trials = Trials()\n",
    "\n",
    "# Fmin will call the objective funbction with selective param set. \n",
    "# The choice of algorithm will narrow the searchspace.\n",
    "\n",
    "best_classifier = fmin(objective, space, algo=tpe.suggest,\n",
    "                       max_evals=50, trials=trials)\n",
    "\n",
    "# Best_params of the best model\n",
    "best_params = space_eval(space, best_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447e8408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Training the best model\n",
    "model = best_params['model'].fit(train_x, train_y)\n",
    "\n",
    "# Predicting with the best model\n",
    "y_pred_train = model.predict(train_x)\n",
    "y_pred_test = model.predict(test_x)\n",
    "\n",
    "# Classification Report \n",
    "print('Training Classification Report for estimator: ',\n",
    "      str(model).split('(')[0])\n",
    "print('\\n', classification_report(train_y, y_pred_train))\n",
    "print('\\n', classification_report(test_y, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e5bf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(models_list)\n",
    "# print(accuracy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffe8cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data frame with accuracies of models\n",
    "\n",
    "model_scores = pd.DataFrame({\n",
    "    'Model Name' : models_list,\n",
    "    'Accuracy' : accuracy_list\n",
    "})\n",
    "\n",
    "file_name = r'C:\\Users\\isarachchand\\Documents\\git\\apf\\output\\cyber_risk\\model_accuracies.csv'\n",
    "\n",
    "model_scores.to_csv(file_name, encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
